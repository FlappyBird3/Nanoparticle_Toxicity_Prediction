{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "813029d2-67df-4291-8aa2-2737f873e735",
   "metadata": {},
   "source": [
    "<center><h2> Nanoparticle Toxicity Prediction/Classification\n",
    "<center><h6> Krish Patel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94062efd-fd0b-41c5-85e3-70250b8b7074",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: https://www.kaggle.com/datasets/ucimachinelearning/nanoparticle-toxicity-dataset\n",
    "\n",
    "#Description: This notebook, using 11 parameters, is meant to predict/classify whether or not a specific NP is toxic.\n",
    "\n",
    "# Variables (n=11):\n",
    "# - class (toxicity outcome) \n",
    "# - NPs (Specific Nanoparticle)\n",
    "# - coresize (size of NP inner diameter)\n",
    "# - hydrosize (hydrodynamic size)\n",
    "# - surfcharge (surface charge of NP)\n",
    "# - surfarea (surface area of NP)\n",
    "# - Ec (charge of NP)\n",
    "# - Expotime (time exposed)\n",
    "# - Dosage (amount of material in use)\n",
    "# - e (energy parameter)\n",
    "# - NOxygen (number of oxygen atoms in NP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece147ef-ed39-4e86-a35a-18748e00e7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import skew, norm, probplot, boxcox, f_oneway\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892ddee4-3ba5-4b98-bac5-fa05ca181d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"nanotox_dataset.csv\")\n",
    "\n",
    "df['NPs'] = df['NPs'].map({'Al2O3': 1, 'CuO': 2, 'Fe2O3': 3, 'TiO2': 4, 'ZnO': 5})\n",
    "df['class'] = df['class'].map({'nonToxic': 0, 'Toxic': 1})\n",
    "\n",
    "df.rename(columns={\"st\": \"Outcome\", \"X1\": \"Curr_Asset\", \"X2\": \"Initial_Goods\", \"X3\": \"Good_Depreciation\", \"X4\": \"EBITDA\", \"X5\": \"Inventory\", \"X6\": \"Net_Income\", \"X7\": \"Receivables\", \"X8\": \"Market_Cap\",\n",
    "                   \"X9\": \"Net_Sales\", \"X10\": \"Total_Asset\", \"X11\": \"LT_Debt\", \"X12\": \"EBIT\", \"X13\": \"Profit\", \"X14\": \"Curr_Liability\", \"X15\": \"Retained_Earnings\", \n",
    "                   \"X16\": \"Total_Revenue\", \"X17\": \"Total_Liability\", \"X18\": \"Total_Operating\"}, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47767c95-5440-4481-b341-9559ff16f693",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd20a9b4-7995-4296-97e1-177351290868",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be178ee-f71d-492d-93c6-0ed800d42969",
   "metadata": {},
   "outputs": [],
   "source": [
    "max = df.max()\n",
    "min = df.min()\n",
    "range = max-min\n",
    "print(range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f68a6b-981b-4329-9a3c-95703c195253",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6e1213-7e04-4f7e-a636-cbaf777e3666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson's correlation (with heatmap)\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.set(font_scale=0.8)\n",
    "plt.rcParams[\"axes.labelsize\"] = 0.5\n",
    "sns.heatmap(df.corr(), annot=True);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb234d7-d235-489f-bf9f-08712c7be8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman's Rank or Spearman's Rho correlation\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.set(font_scale=0.8)\n",
    "plt.rcParams[\"axes.labelsize\"] = 0.5\n",
    "sns.heatmap(df.corr(method='spearman'), annot=True,); # nonparametric correlation\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "812d81f3-1715-4dbd-ad22-1ee53c282ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Phi K correlations for all variables\n",
    "\n",
    "# ! pip install phik\n",
    "#import phik\n",
    "#from phik import resources, report\n",
    "\n",
    "# df.phik_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6335e263-6761-4c24-9b1f-72376428c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from phik.report import plot_correlation_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Define a custom colormap with lighter and darker versions of the extracted colors\n",
    "custom_sea_green_cmap = mcolors.LinearSegmentedColormap.from_list(\n",
    "    \"CustomSeaGreen\", [\"black\", \"orange\", \"white\"]\n",
    ")\n",
    "\n",
    "# Compute the Phi K matrix\n",
    "phik_overview = df.phik_matrix()\n",
    "\n",
    "# Presenting Phi K data as a heatmap with the custom sea green color theme\n",
    "plot_correlation_matrix(phik_overview.values, \n",
    "                        x_labels=phik_overview.columns, \n",
    "                        y_labels=phik_overview.index, \n",
    "                        vmin=0, vmax=1, color_map=custom_sea_green_cmap, \n",
    "                        title=r\"correlation $\\phi_K$\", \n",
    "                        fontsize_factor=1, \n",
    "                        figsize=(10, 9))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c7c65f-038d-4d9e-9eed-dea364471d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.countplot(data=df, x='class')\n",
    "\n",
    "# Annotate the bars with counts and percentages\n",
    "total = len(df['class'])\n",
    "for p in ax.patches:\n",
    "    count = p.get_height()\n",
    "    percentage = '{:.1f}%'.format(100 * count / total)\n",
    "    x = p.get_x() + p.get_width() / 2\n",
    "    y = p.get_height()\n",
    "    ax.text(x, y, f'{count}\\n{percentage}', ha='center', va='bottom')\n",
    "\n",
    "plt.title('Count of Toxic Particles vs Non-Toxic Particles')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1877a1d2-da4f-413e-b232-378e7d569115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection using boxplots (for all I/R variables)\n",
    "plt.figure(figsize=(20,30))\n",
    "\n",
    "for i, variable in enumerate(df): #enumerate is a built-in function in python that allows you to keep track of the number of iterations (loops) in a loop\n",
    "                     plt.subplot(5,4,i+1) #provides a way to plot multiple plots on a single figure\n",
    "                     plt.boxplot(df[variable],whis=1.5)\n",
    "                     plt.tight_layout()\n",
    "                     plt.title(variable)\n",
    "                    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb1fe76-0984-4a01-bb5c-2f61222e4a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balancing data to ensure no bias\n",
    "\n",
    "# Identify the indices of rows where Output = 0\n",
    "index_to_delete = df[df['class'] == 1].sample(n=71, random_state=1).index\n",
    "\n",
    "# Drop the identified rows\n",
    "balanced_df = df.drop(index_to_delete)\n",
    "\n",
    "\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "ax = sns.countplot(data=balanced_df, x='class')\n",
    "\n",
    "# Annotate the bars with counts and percentages\n",
    "total = len(balanced_df['class'])\n",
    "for p in ax.patches:\n",
    "    count = p.get_height()\n",
    "    percentage = '{:.1f}%'.format(100 * count / total)\n",
    "    x = p.get_x() + p.get_width() / 2\n",
    "    y = p.get_height()\n",
    "    ax.text(x, y, f'{count}\\n{percentage}', ha='center', va='bottom')\n",
    "\n",
    "plt.title('Count of class Variable in Balanced Dataset')\n",
    "plt.xlabel('class')\n",
    "plt.ylabel('Count')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fca7ba0-7aa0-457b-80b8-0019a5255800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier detection using boxplots\n",
    "\n",
    "plt.figure(figsize=(20,30))\n",
    "\n",
    "for i, variable in enumerate(balanced_df):\n",
    "                     plt.subplot(5,4,i+1)\n",
    "                     plt.boxplot(balanced_df[variable],whis=1.5)\n",
    "                     plt.tight_layout()\n",
    "                     plt.title(variable)\n",
    "                    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8a5b5d-d744-4043-a7b7-a9c00bb5c665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use flooring and capping method\n",
    "def treat_outliers(df,col):\n",
    "    Q1=df[col].quantile(0.25) \n",
    "    Q3=df[col].quantile(0.75) \n",
    "    IQR=Q3-Q1\n",
    "    Lower_Whisker = Q1 - 1.5*IQR \n",
    "    Upper_Whisker = Q3 + 1.5*IQR\n",
    "    df[col] = np.clip(df[col], Lower_Whisker, Upper_Whisker)                                                            \n",
    "    return df\n",
    "\n",
    "def treat_outliers_all(df, col_list):\n",
    "    for c in col_list:\n",
    "        df = treat_outliers(df,c)\n",
    "    return df\n",
    "\n",
    "\n",
    "numerical_col = balanced_df.select_dtypes(include=np.number).columns.tolist()\n",
    "balanced_df = treat_outliers_all(balanced_df, numerical_col)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(20,30))\n",
    "\n",
    "for i, variable in enumerate(numerical_col):\n",
    "                     plt.subplot(5,4,i+1)\n",
    "                     plt.boxplot(balanced_df[variable],whis=1.5)\n",
    "                     plt.tight_layout()\n",
    "                     plt.title(variable)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d234d25-1e9f-4536-b451-97ce1064900b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn import datasets\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn import linear_model\n",
    "import scipy.stats as stats\n",
    "from scipy.stats import stats, norm, skew\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Make copy of data\n",
    "data=balanced_df.copy()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce268a71-57bd-4bfb-a507-0568b72f06e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('class',axis=1)    # Features\n",
    "y = data['class'].astype('int64') # Labels\n",
    "# making sure target data are integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e93c57f-e933-4752-9536-ef70852f7468",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting data into training and test set:\n",
    "X_train, X_test, y_train, y_test =train_test_split(X, y, test_size=0.3, random_state=1)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a3f92d-5ab0-454b-a3d5-22419da53a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#decision tree\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "model = DecisionTreeClassifier(criterion='gini',class_weight={0:0.15,1:0.85},random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962856c8-4aae-465e-b432-207e2d4a627a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574d2b05-b19f-4276-867f-11d7d2791c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Function to calculate different metric scores - Accuracy, Recall and Precision\n",
    "def get_metrics_score(model,flag=True):\n",
    "    # defining an empty list to store train and test results\n",
    "    score_list=[] \n",
    "    \n",
    "    pred_train = model.predict(X_train)\n",
    "    pred_test = model.predict(X_test)\n",
    "    \n",
    "    train_acc = model.score(X_train,y_train)\n",
    "    test_acc = model.score(X_test,y_test)\n",
    "    \n",
    "    train_recall = metrics.recall_score(y_train,pred_train)\n",
    "    test_recall = metrics.recall_score(y_test,pred_test)\n",
    "    # Recall = minimizes false negatives\n",
    "    \n",
    "    train_precision = metrics.precision_score(y_train,pred_train)\n",
    "    test_precision = metrics.precision_score(y_test,pred_test)\n",
    "    # Precision = minimizes false positives\n",
    "    \n",
    "    score_list.extend((train_acc,test_acc,train_recall,test_recall,train_precision,test_precision))\n",
    "        \n",
    "    if flag == True: \n",
    "        print(\"Accuracy on training set : \",model.score(X_train,y_train))\n",
    "        print(\"Accuracy on test set : \",model.score(X_test,y_test))\n",
    "        print(\"Recall on training set : \",metrics.recall_score(y_train,pred_train))\n",
    "        print(\"Recall on test set : \",metrics.recall_score(y_test,pred_test))\n",
    "        print(\"Precision on training set : \",metrics.precision_score(y_train,pred_train))\n",
    "        print(\"Precision on test set : \",metrics.precision_score(y_test,pred_test))\n",
    "    \n",
    "    return score_list # returns the list with train and test scores\n",
    "    \n",
    "def make_confusion_matrix(model,y_actual,labels=[1, 0]):\n",
    "    y_predict = model.predict(X_test)\n",
    "    cm=metrics.confusion_matrix( y_actual, y_predict, labels=[0, 1])\n",
    "    df_cm = pd.DataFrame(cm, index = [i for i in [\"Actual - No\",\"Actual - Yes\"]],\n",
    "                  columns = [i for i in ['Predicted - No','Predicted - Yes']])\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cm.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         cm.flatten()/np.sum(cm)]\n",
    "    labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n",
    "              zip(group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sns.heatmap(df_cm, annot=labels,fmt='')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca75a364-1efe-4c2a-b6a0-e6a033583e8b",
   "metadata": {},
   "source": [
    "<center><h2> Normal Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc1717f-9b78-4685-b21d-4825227406a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "d_tree = DecisionTreeClassifier(random_state=1)\n",
    "d_tree.fit(X_train,y_train)\n",
    "\n",
    "# Calculate metrics\n",
    "get_metrics_score(d_tree)\n",
    "\n",
    "# Create the confusion matrix\n",
    "make_confusion_matrix(d_tree,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67717eb4-1c39-4134-b0a7-df8ad6ba0b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "\n",
    "column_names = list(data.columns)\n",
    "column_names.remove('class')  # As this is the DV                \n",
    "feature_names = column_names\n",
    "print(feature_names)\n",
    "\n",
    "plt.figure(figsize=(20,30))\n",
    "out = tree.plot_tree(model,feature_names=feature_names,filled=True,fontsize=9,node_ids=False,class_names=None,)\n",
    "# Code below will add arrows to the decision tree split if they are missing\n",
    "for o in out:\n",
    "     arrow = o.arrow_patch\n",
    "     if arrow is not None:\n",
    "        arrow.set_edgecolor('black')\n",
    "        arrow.set_linewidth(1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf59913-a750-4edc-8505-e713ab04ca95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Grid Search technique for hyperparameter tuning\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "dtree_tuned = DecisionTreeClassifier(class_weight={0:0.35,1:0.65},random_state=1)\n",
    "dtree_estimator = DecisionTreeClassifier(class_weight={0:0.35,1:0.65},random_state=1)\n",
    "parameters = {'max_depth': np.arange(2,10), \n",
    "              'min_samples_leaf': [5, 7, 10, 15],\n",
    "              'max_leaf_nodes' : [2, 3, 5, 10,15],\n",
    "              'min_impurity_decrease': [0.0001,0.001,0.01,0.1]\n",
    "             }\n",
    "\n",
    "scorer = metrics.make_scorer(metrics.recall_score)\n",
    "\n",
    "grid_obj = GridSearchCV(dtree_estimator, parameters, scoring=scorer,n_jobs=-1)\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "dtree_tuned = grid_obj.best_estimator_\n",
    "\n",
    "dtree_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11006ee4-728c-42fa-894c-457fab43867a",
   "metadata": {},
   "source": [
    "<center><h2> Tuned Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71adddd-72c2-4bbe-a089-c8438610179a",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics_score(dtree_tuned)\n",
    "\n",
    "make_confusion_matrix(dtree_tuned,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1a8740-a35f-487a-a219-b1e45f62e9e5",
   "metadata": {},
   "source": [
    "<center><h2> Normal Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1197460-a0c4-4dad-99fe-10979ebee2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.colors as mcolors\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Define a custom colormap with lighter and darker versions of the extracted colors\n",
    "custom_sea_green_cmap = mcolors.LinearSegmentedColormap.from_list(\n",
    "    \"CustomSeaGreen\", [\"#D1EAE7\", \"#A3D3CD\", \"#388062\", \"#264D3F\"]\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "rf_estimator = RandomForestClassifier(random_state=1)\n",
    "rf_estimator.fit(X_train, y_train)\n",
    "\n",
    "# Calculate metrics\n",
    "get_metrics_score(rf_estimator)\n",
    "\n",
    "# Function to make confusion matrix\n",
    "def make_confusion_matrix(model, y_actual, X_test):\n",
    "    y_predict = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_actual, y_predict, labels=[0, 1])\n",
    "    df_cm = pd.DataFrame(cm, index=[\"Actual - No\", \"Actual - Yes\"],\n",
    "                         columns=[\"Predicted - No\", \"Predicted - Yes\"])\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in cm.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in cm.flatten() / np.sum(cm)]\n",
    "    labels = [f\"{v1}\\n{v2}\" for v1, v2 in zip(group_counts, group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2, 2)\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(df_cm, annot=labels, fmt='', cmap=custom_sea_green_cmap, cbar=True)\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    \n",
    "    print (pd.DataFrame(model.feature_importances_, columns = [\"Imp\"], index = X_train.columns).sort_values(by = 'Imp', ascending = False))\n",
    "\n",
    "\n",
    "# Create the confusion matrix\n",
    "make_confusion_matrix(rf_estimator, y_test, X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6c8679-d42c-4c2b-a91a-4ef07ccbad20",
   "metadata": {},
   "source": [
    "<center><h2> Normal Bagging Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e84150c-c642-4726-bae3-d56cc3b9e0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for different classifiers\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "# Libraries for model tuning and evaluation metrics\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Fit the model\n",
    "bagging_classifier = BaggingClassifier(random_state=1)\n",
    "bagging_classifier.fit(X_train,y_train)\n",
    "\n",
    "# Calculate metrics\n",
    "get_metrics_score(bagging_classifier)\n",
    "\n",
    "def make_confusion_matrix(model,y_actual,labels=[1, 0]):\n",
    "    y_predict = model.predict(X_test)\n",
    "    cm=metrics.confusion_matrix( y_actual, y_predict, labels=[0, 1])\n",
    "    df_cm = pd.DataFrame(cm, index = [i for i in [\"Actual - No\",\"Actual - Yes\"]],\n",
    "                  columns = [i for i in ['Predicted - No','Predicted - Yes']])\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cm.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         cm.flatten()/np.sum(cm)]\n",
    "    labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n",
    "              zip(group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sns.heatmap(df_cm, annot=labels,fmt='')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "# Create the confusion matrix\n",
    "make_confusion_matrix(bagging_classifier,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bff3e5bb-df1b-4829-b1a8-7e458e8e43b7",
   "metadata": {},
   "source": [
    "<center><h2> Tuned Bagging Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45144cd3-04a4-4618-9050-976425254acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "bagging_tuned = BaggingClassifier(random_state=1)\n",
    "\n",
    "parameters = {'max_samples': [0.7,0.8,0.9,1], \n",
    "              'max_features': [0.7,0.8,0.9,1],\n",
    "              'n_estimators' : [10,20,30,40,50],\n",
    "             }\n",
    "\n",
    "acc_scorer = metrics.make_scorer(metrics.recall_score)\n",
    "\n",
    "grid_obj = GridSearchCV(bagging_tuned, parameters, scoring=acc_scorer,cv=5)\n",
    "grid_obj = grid_obj.fit(X_train, y_train)\n",
    "\n",
    "bagging_tuned = grid_obj.best_estimator_\n",
    "\n",
    "bagging_tuned.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d009dd-aa72-444b-b7ba-a789dfa55485",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_metrics_score(bagging_tuned)\n",
    "\n",
    "def make_confusion_matrix(model,y_actual,labels=[1, 0]):\n",
    "    y_predict = model.predict(X_test)\n",
    "    cm=metrics.confusion_matrix( y_actual, y_predict, labels=[0, 1])\n",
    "    df_cm = pd.DataFrame(cm, index = [i for i in [\"Actual - No\",\"Actual - Yes\"]],\n",
    "                  columns = [i for i in ['Predicted - No','Predicted - Yes']])\n",
    "    group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                cm.flatten()]\n",
    "    group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                         cm.flatten()/np.sum(cm)]\n",
    "    labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n",
    "              zip(group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    plt.figure(figsize = (10,7))\n",
    "    sns.heatmap(df_cm, annot=labels,fmt='')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.show()\n",
    "    \n",
    "make_confusion_matrix(bagging_tuned,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199f8675-a976-4faf-be4e-64cb40adab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the models to compare\n",
    "models = [d_tree, dtree_tuned, rf_estimator, bagging_classifier, bagging_tuned]\n",
    "\n",
    "# Define empty lists to add results\n",
    "acc_train = []\n",
    "acc_test = []\n",
    "recall_train = []\n",
    "recall_test = []\n",
    "precision_train = []\n",
    "precision_test = []\n",
    "f1_train = []\n",
    "f1_test = []\n",
    "\n",
    "# Function to calculate F1 score\n",
    "def calculate_f1(precision, recall):\n",
    "    if precision + recall == 0:\n",
    "        return 0\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "# Loop through all identified models to get the metrics score (i.e., Accuracy, Recall, Precision, and F1)\n",
    "for model in models:\n",
    "    j = get_metrics_score(model, False)\n",
    "    print(f\"Metrics for model {model}: {j}\")  # Debug print\n",
    "    if len(j) != 6:\n",
    "        raise ValueError(f\"Expected 6 metrics, but got {len(j)} for model {model}.\")\n",
    "    \n",
    "    acc_train.append(j[0])\n",
    "    acc_test.append(j[1])\n",
    "    recall_train.append(j[2])\n",
    "    recall_test.append(j[3])\n",
    "    precision_train.append(j[4])\n",
    "    precision_test.append(j[5])\n",
    "    \n",
    "    # Calculate F1 scores\n",
    "    f1_train.append(calculate_f1(j[4], j[2]))\n",
    "    f1_test.append(calculate_f1(j[5], j[3]))\n",
    "\n",
    "comparison_frame = pd.DataFrame({\n",
    "    'Model': ['Decision Tree', 'Tuned Decision Tree', 'Random Forest', 'Bagging Classifier', 'Tuned Bagging Classifier'], \n",
    "    'Train_Accuracy': acc_train,\n",
    "    'Test_Accuracy': acc_test, \n",
    "    'Train_Recall': recall_train,\n",
    "    'Test_Recall': recall_test,\n",
    "    'Train_Precision': precision_train,\n",
    "    'Test_Precision': precision_test,\n",
    "    'F1_Train': f1_train, \n",
    "    'F1_Test': f1_test\n",
    "})\n",
    "\n",
    "# Sort models in decreasing order of test F1 score\n",
    "comparison_frame.sort_values(by='F1_Test', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "492e2da0-171f-46bd-b459-de04e4df4f02",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3788eb6f-93d6-4076-b718-74190f91874d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
